{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dascient/face-reader?scriptVersionId=292632848\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# üß† DaScient Intelligence Academy: Hyper-Sensory Emotion Recognition\n### *Engineering Emotional Intelligence into Computer Vision*\n\n**Author:** [DaScient](https://www.kaggle.com/dascient)  \n**Context:** Part of the DaScient Intelligence Academy‚Ñ¢ Series.\n\n## 1. Project Overview\nStandard emotion detection identifies \"Happy\" or \"Sad.\" But human intelligence is nuanced. A smirk is not just \"Happy\"; it is \"Happy + Contempt.\" This notebook builds a **Super-Intelligent Emotion Reader** that:\n1. Uses a pre-trained **Vision Transformer (ViT)** for state-of-the-art feature extraction.\n2. Implements a custom **\"Nuance Layer\"** to detect compound emotions.\n3. Provides a clean, modular inference pipeline ready for video deployment.\n\n### Tech Stack\n* **Core:** PyTorch, Timm (PyTorch Image Models)\n* **Architecture:** ViT-Base-Patch16-224 (Transfer Learning)\n* **Dataset:** FER-2013 (Facial Expression Recognition)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# üõ†Ô∏è System Setup & Hyper-Tech Configuration\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms, models\nimport timm  # PyTorch Image Models for State-of-the-Art architectures\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\nfrom tqdm.notebook import tqdm\n\n# Set device to GPU (Standard for Deep Learning)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"üöÄ DaScient System Online. Compute Device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T00:08:10.539348Z","iopub.execute_input":"2026-01-19T00:08:10.540035Z","iopub.status.idle":"2026-01-19T00:08:28.309562Z","shell.execute_reply.started":"2026-01-19T00:08:10.539999Z","shell.execute_reply":"2026-01-19T00:08:28.30858Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"üöÄ DaScient System Online. Compute Device: cpu\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"class DaScientEmotionNet(nn.Module):\n    def __init__(self, num_classes=7):\n        super(DaScientEmotionNet, self).__init__()\n        # We use a Vision Transformer (ViT) instead of a standard CNN\n        # ViTs are better at capturing global context (how eyes relate to mouth)\n        self.base_model = timm.create_model('vit_base_patch16_224', pretrained=True)\n        \n        # Replace the head for our specific emotion classes\n        self.base_model.head = nn.Sequential(\n            nn.Linear(self.base_model.head.in_features, 512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, x):\n        return self.base_model(x)\n\n# Initialize Model\nmodel = DaScientEmotionNet(num_classes=7)\nmodel.to(device)\nprint(\"üß† Neural Architecture Loaded: Vision Transformer (ViT-Base)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T00:08:28.313831Z","iopub.execute_input":"2026-01-19T00:08:28.314164Z","iopub.status.idle":"2026-01-19T00:08:32.605783Z","shell.execute_reply.started":"2026-01-19T00:08:28.314133Z","shell.execute_reply":"2026-01-19T00:08:32.605004Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4b9ebad502c4dbb8da539e8ae1ea49f"}},"metadata":{}},{"name":"stdout","text":"üß† Neural Architecture Loaded: Vision Transformer (ViT-Base)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def interpret_emotional_nuance(probabilities, classes):\n    \"\"\"\n    Parses raw probabilities to find 'Compound Emotions' \n    (e.g., Happily Surprised, Anxiously Sad).\n    \"\"\"\n    # Get top 2 emotions\n    top2_prob, top2_idx = torch.topk(probabilities, 2)\n    primary_emotion = classes[top2_idx[0]]\n    secondary_emotion = classes[top2_idx[1]]\n    primary_conf = top2_prob[0].item()\n    secondary_conf = top2_prob[1].item()\n    \n    # Logic for Compound Emotions\n    nuance = primary_emotion\n    \n    if primary_conf < 0.9 and secondary_conf > 0.15:\n        # If the model isn't 90% sure, we look at the mix\n        if primary_emotion == 'Happy' and secondary_emotion == 'Surprise':\n            nuance = \"Ecstatic (Happy + Surprise)\"\n        elif primary_emotion == 'Sad' and secondary_emotion == 'Fear':\n            nuance = \"Despairing (Sad + Fear)\"\n        elif primary_emotion == 'Angry' and secondary_emotion == 'Disgust':\n            nuance = \"Contemptuous (Angry + Disgust)\"\n        elif primary_emotion == 'Neutral' and secondary_emotion == 'Happy':\n            nuance = \"Content / Smirking\"\n            \n    return {\n        \"Dominant\": primary_emotion,\n        \"Nuance\": nuance,\n        \"Confidence\": f\"{primary_conf*100:.2f}%\",\n        \"Secondary Trace\": f\"{secondary_emotion} ({secondary_conf*100:.2f}%)\"\n    }\n\nprint(\"‚ú® Emotional Intelligence Logic Module: Active\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T00:08:32.607665Z","iopub.execute_input":"2026-01-19T00:08:32.607957Z","iopub.status.idle":"2026-01-19T00:08:32.616142Z","shell.execute_reply.started":"2026-01-19T00:08:32.607933Z","shell.execute_reply":"2026-01-19T00:08:32.615267Z"}},"outputs":[{"name":"stdout","text":"‚ú® Emotional Intelligence Logic Module: Active\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## üöÄ Deployment & Future\nThis notebook demonstrates that **DaScient Intelligence** is not just about raw data, but about *interpreting* that data into human-relevant context. \n\n**Next Steps:**\n1.  Connect this backend to a live video feed (OpenCV).\n2.  Integrate with a Text-to-Speech engine to allow the AI to *verbalize* what it sees.","metadata":{}},{"cell_type":"code","source":"# en fin","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}